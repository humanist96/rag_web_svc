"""
ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê²€ì‚¬ ì‹œìŠ¤í…œ
ì§ˆë¬¸ì´ ì—…ë¡œë“œëœ ë¬¸ì„œì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ ê²€ì¦
"""

import re
from typing import List, Dict, Tuple, Optional
import logging

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    
try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    KONLPY_AVAILABLE = False

logger = logging.getLogger(__name__)

class ContextRelevanceChecker:
    """ì§ˆë¬¸ê³¼ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì˜ ê´€ë ¨ì„±ì„ ê²€ì‚¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.okt = Okt() if KONLPY_AVAILABLE else None
        self.general_questions = [
            # ì¼ë°˜ì ì¸ ì§ˆë¬¸ íŒ¨í„´
            r"ì•ˆë…•|ë°˜ê°€|í•˜ì´|í—¬ë¡œ|hello|hi",
            r"ë‚ ì”¨|ê¸°ì˜¨|ë¹„|ëˆˆ|ë§‘|íë¦¼",
            r"ì˜¤ëŠ˜|ë‚´ì¼|ì–´ì œ|ìš”ì¼|ë‚ ì§œ|ì‹œê°„",
            r"ëˆ„êµ¬|ë‹¹ì‹ |ë„ˆ|AI|ì¸ê³µì§€ëŠ¥|ì±—ë´‡",
            r"ë­í•´|ë¬´ì—‡ì„|ë­˜|ì–´ë–»ê²Œ ì§€ë‚´",
            r"ê³ ë§ˆì›Œ|ê°ì‚¬|ë•¡í|thanks",
            r"ë¯¸ì•ˆ|ì£„ì†¡|sorry",
            r"ë†ë‹´|ì¬ë¯¸ìˆ|ì›ƒê¸´",
            r"ì‚¬ë‘|ì¢‹ì•„|ì‹«ì–´|ê°ì •",
            r"ê²Œì„|ì˜í™”|ë“œë¼ë§ˆ|ìŒì•…|ì—°ì˜ˆì¸",
            r"ìš”ë¦¬|ìŒì‹|ë ˆì‹œí”¼|ë§›ì§‘",
            r"ìš´ë™|ìŠ¤í¬ì¸ |ì¶•êµ¬|ì•¼êµ¬|ë†êµ¬",
            r"ì£¼ì‹|ì½”ì¸|ë¹„íŠ¸ì½”ì¸|íˆ¬ì",
            r"ì‡¼í•‘|êµ¬ë§¤|íŒë§¤|ê°€ê²©",
        ]
        
        self.document_related_keywords = [
            # ë¬¸ì„œ ê´€ë ¨ í‚¤ì›Œë“œ
            "ë¬¸ì„œ", "íŒŒì¼", "ë‚´ìš©", "ìë£Œ", "ë°ì´í„°",
            "í˜ì´ì§€", "ë‹¨ë½", "ì„¹ì…˜", "ì±•í„°", "ë¶€ë¶„",
            "ì„¤ëª…", "ì •ì˜", "ì˜ë¯¸", "ëœ»", "í•´ì„",
            "ë¶„ì„", "ìš”ì•½", "ì •ë¦¬", "í•µì‹¬", "ì¤‘ìš”",
            "ì°¾ì•„", "ê²€ìƒ‰", "í™•ì¸", "ì•Œë ¤", "ë³´ì—¬",
            "ì–´ë””", "ì–¸ê¸‰", "ë‚˜ì™€", "ìˆëŠ”", "ê´€ë ¨"
        ]
        
        self.off_topic_responses = [
            "ì£„ì†¡í•©ë‹ˆë‹¤. ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”. ğŸ”",
            "ì´ ì§ˆë¬¸ì€ í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œì™€ ê´€ë ¨ì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ì ì„ ë¬¼ì–´ë´ ì£¼ì„¸ìš”. ğŸ“„",
            "ì œê°€ ë‹µë³€ë“œë¦´ ìˆ˜ ìˆëŠ” ë²”ìœ„ëŠ” ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œ ì œí•œë©ë‹ˆë‹¤. ë¬¸ì„œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ğŸ“š",
            "í˜„ì¬ ì œê³µëœ ë¬¸ì„œ ì™¸ì˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì—ëŠ” ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. ğŸ’¡",
            "ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ğŸ¯"
        ]
    
    def extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            keywords = []
            
            if self.okt and KONLPY_AVAILABLE:
                # í˜•íƒœì†Œ ë¶„ì„
                tokens = self.okt.nouns(text)
                # 2ê¸€ì ì´ìƒì¸ ëª…ì‚¬ë§Œ ì¶”ì¶œ
                keywords = [token for token in tokens if len(token) >= 2]
            else:
                # ê°„ë‹¨í•œ ë‹¨ì–´ ì¶”ì¶œ (konlpy ì—†ì„ ë•Œ)
                # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
                korean_words = re.findall(r'[ê°€-í£]{2,}', text)
                keywords.extend(korean_words)
            
            # ì˜ì–´ ë‹¨ì–´ ì¶”ì¶œ
            english_words = re.findall(r'\b[A-Za-z]{3,}\b', text)
            keywords.extend([word.lower() for word in english_words])
            
            return list(set(keywords))
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def is_general_question(self, question: str) -> bool:
        """ì¼ë°˜ì ì¸ ëŒ€í™”ì„± ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
        question_lower = question.lower()
        
        for pattern in self.general_questions:
            if re.search(pattern, question_lower):
                return True
        
        # ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼
        if len(question.strip()) < 10:
            return True
        
        # ë¬¸ì„œ ê´€ë ¨ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼
        has_doc_keyword = any(keyword in question for keyword in self.document_related_keywords)
        if not has_doc_keyword and len(question.split()) < 5:
            return True
        
        return False
    
    def calculate_relevance_score(self, question: str, document_chunks: List[str]) -> float:
        """ì§ˆë¬¸ê³¼ ë¬¸ì„œ ì²­í¬ë“¤ ê°„ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if not document_chunks:
                return 0.0
            
            # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
            question_keywords = self.extract_keywords(question)
            if not question_keywords:
                return 0.0
            
            # ë¬¸ì„œ ì „ì²´ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ìƒìœ„ 100ê°œ ì²­í¬ë§Œ ì‚¬ìš©)
            sample_chunks = document_chunks[:100]
            doc_text = " ".join(sample_chunks)
            doc_keywords = self.extract_keywords(doc_text)
            
            if not doc_keywords:
                return 0.0
            
            # í‚¤ì›Œë“œ êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
            common_keywords = set(question_keywords) & set(doc_keywords)
            relevance_score = len(common_keywords) / len(question_keywords) if question_keywords else 0
            
            # TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
            if SKLEARN_AVAILABLE:
                try:
                    all_texts = [question] + sample_chunks[:20]  # ì§ˆë¬¸ + ìƒìœ„ 20ê°œ ì²­í¬
                    vectorizer = TfidfVectorizer(max_features=100)
                    tfidf_matrix = vectorizer.fit_transform(all_texts)
                    
                    # ì§ˆë¬¸ê³¼ ê° ì²­í¬ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                    question_vector = tfidf_matrix[0:1]
                    chunk_vectors = tfidf_matrix[1:]
                    similarities = cosine_similarity(question_vector, chunk_vectors).flatten()
                    
                    # ìµœëŒ€ ìœ ì‚¬ë„
                    max_similarity = np.max(similarities) if len(similarities) > 0 else 0
                    
                    # ì¢…í•© ì ìˆ˜ (í‚¤ì›Œë“œ ë§¤ì¹­ 40% + ìœ ì‚¬ë„ 60%)
                    final_score = (relevance_score * 0.4) + (max_similarity * 0.6)
                    
                    return float(final_score)
                    
                except Exception as e:
                    logger.error(f"TF-IDF ê³„ì‚° ì‹¤íŒ¨: {e}")
                    return relevance_score
            else:
                # sklearn ì—†ì„ ë•ŒëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ë§Œ ì‚¬ìš©
                return relevance_score
                
        except Exception as e:
            logger.error(f"ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def check_relevance(self, question: str, document_chunks: List[str], 
                       threshold: float = 0.15) -> Tuple[bool, float, Optional[str]]:
        """
        ì§ˆë¬¸ì˜ ê´€ë ¨ì„± ê²€ì‚¬
        
        Returns:
            - is_relevant: ê´€ë ¨ì„± ì—¬ë¶€
            - score: ê´€ë ¨ì„± ì ìˆ˜
            - rejection_message: ê±°ë¶€ ë©”ì‹œì§€ (ê´€ë ¨ ì—†ì„ ê²½ìš°)
        """
        # ë¹ˆ ì§ˆë¬¸ ì²˜ë¦¬
        if not question or not question.strip():
            return False, 0.0, "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        # ì¼ë°˜ì ì¸ ëŒ€í™”ì„± ì§ˆë¬¸ í•„í„°ë§
        if self.is_general_question(question):
            import random
            return False, 0.0, random.choice(self.off_topic_responses)
        
        # ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        if not document_chunks:
            return False, 0.0, "ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”. ğŸ“"
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance_score = self.calculate_relevance_score(question, document_chunks)
        
        # ì„ê³„ê°’ ê¸°ë°˜ íŒë‹¨
        is_relevant = relevance_score >= threshold
        
        if not is_relevant:
            # ì ìˆ˜ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ì‘ë‹µ
            import random
            if relevance_score < 0.05:
                # ë§¤ìš° ë‚®ì€ ê´€ë ¨ì„±
                message = random.choice(self.off_topic_responses)
            elif relevance_score < 0.1:
                # ë‚®ì€ ê´€ë ¨ì„±
                message = "ì´ ì§ˆë¬¸ì€ ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì£¼ì œì™€ ë‹¤ì†Œ ê±°ë¦¬ê°€ ìˆì–´ ë³´ì…ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ê³¼ ë” ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”. ğŸ¤”"
            else:
                # ê²½ê³„ì„ ìƒì˜ ê´€ë ¨ì„±
                message = "ì§ˆë¬¸ì´ ë¬¸ì„œ ë‚´ìš©ê³¼ ì•½ê°„ì˜ ì—°ê´€ì„±ì€ ìˆì§€ë§Œ, ë” êµ¬ì²´ì ìœ¼ë¡œ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ“"
            
            return False, relevance_score, message
        
        return True, relevance_score, None
    
    def enhance_question_with_context(self, question: str, is_relevant: bool, score: float) -> str:
        """ì§ˆë¬¸ì— ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€"""
        if is_relevant:
            if score > 0.5:
                prefix = "[ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸] "
            elif score > 0.3:
                prefix = "[ë¶€ë¶„ì  ê´€ë ¨ ì§ˆë¬¸] "
            else:
                prefix = "[í™•ì¸ í•„ìš” ì§ˆë¬¸] "
            
            return f"{prefix}{question}"
        
        return question
    
    def get_contextual_hints(self, document_chunks: List[str], max_hints: int = 5) -> List[str]:
        """ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ íŒíŠ¸ ìƒì„±"""
        if not document_chunks:
            return []
        
        # ë¬¸ì„œì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        sample_text = " ".join(document_chunks[:30])
        keywords = self.extract_keywords(sample_text)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ê¸°ë°˜ íŒíŠ¸ ìƒì„±
        hints = []
        top_keywords = keywords[:10]
        
        if top_keywords:
            hints.extend([
                f"{keyword}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”" for keyword in top_keywords[:2]
            ])
            hints.extend([
                f"{keyword}ì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?" for keyword in top_keywords[2:4]
            ])
            hints.append(f"ë¬¸ì„œì—ì„œ {top_keywords[0]}ì™€ {top_keywords[1]}ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”")
        
        # ì¼ë°˜ì ì¸ ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ ì¶”ê°€
        general_hints = [
            "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
            "ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í¬ì¸íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ì´ ìë£Œì˜ ì£¼ìš” ê²°ë¡ ì„ ì•Œë ¤ì£¼ì„¸ìš”"
        ]
        
        hints.extend(general_hints)
        
        return hints[:max_hints]